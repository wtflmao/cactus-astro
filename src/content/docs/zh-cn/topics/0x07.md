---
title: 0x07 - 从全量微调到 LoRA
description: 从全量微调到 LoRA
time: 2025-08-08
---

## 引言

这份笔记旨在系统性地梳理大语言模型微调的核心概念，为所有学习者提供一份详实、独立的参考资料。内容从基础的微调范式出发，深入探讨其面临的现实瓶颈，并最终聚焦于当前最主流的高效微调技术 LoRA 的核心原理与实践细节。

---

## 1. 微调的起点：全量微调 (Full Fine-Tuning)

### 1.1 核心思想与直觉路径

对于一个已经经过大规模数据预训练的“通才”基础模型（Base Model），若想使其“专业化”以适应特定任务（如编程、法律咨询等），最直接的方法便是进行全量微调。

其路径非常符合直觉：获取一个开源的基础模型，准备一份针对目标任务的高质量数据集，然后以这些新数据继续训练模型的几乎所有参数。这个过程旨在将模型的知识体系和行为模式，向特定领域进行深度迁移。

### 1.2 实践考量与严峻挑战

**灾难性遗忘 (Catastrophic Forgetting)**：这是一个在全量微调中普遍存在的现象。当模型全力学习新领域的知识时，其优化算法（如梯度下降）会调整权重以最小化新任务的损失，这可能导致模型在预训练阶段学到的通用知识和能力被“覆盖”或“遗忘”。
> **缓解策略**：一个业界常用且有效的缓解策略是，在新的专业数据集中，有意识地掺入一小部分（如 5-10%）通用的、多样化的数据。这有助于模型在学习新技能的同时，“温习”其原有的通用能力，维持知识的广度。

**巨大的计算资源瓶颈**：这是阻碍全量微调普及的根本原因。以微调一个 80 亿（8B）参数的模型为例，其对硬件的要求极高：

* **模型权重存储**：若以 16 位浮点数（FP16）存储，模型参数本身就需要约 16 GB 的显存（VRAM）。
* **梯度存储**：在反向传播中，需要为每一个待更新的参数计算并存储梯度，这部分开销大致与模型权重相当，即额外的 16 GB。
* **优化器状态**：广泛使用的 AdamW 等优化器，为了高效更新参数，需要存储动量（Momentum）和方差（Variance）等状态信息，这通常需要两倍于模型参数量的存储空间，即约 32 GB。

综合来看，一个 8B 模型的全量微调，至少需要 `16 + 16 + 32 = 64 GB` 的显存。这个门槛远非顶级消费级显卡（如 24 GB VRAM）所能及。工业界虽可通过分布式训练（多卡并行）或 CPU 卸载等技术实现，但其成本和复杂性对大多数开发者和研究者而言都过高。

## 2. 范式革命：参数高效微调 (PEFT)

面对全量微调的困境，研究界转换思路，提出了一个颠覆性的问题，由此催生了 `参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）` 这一技术家族。

> **核心问题**：“为了让一位专家掌握一项新技能，我们是否真的需要重塑他的整个大脑（更新全部参数）？还是说，只需在他大脑的特定区域，植入一个微小的‘知识补丁’就已足够？”

PEFT 的核心哲学正是后者。它主张冻结（Freeze）预训练模型绝大部分的原始参数，只添加或修改极少数（通常不到 1%）的可训练参数，从而以极低的成本实现高效微调。

## 3. PEFT 的明星方案：低秩适应 (LoRA)

`低秩适应（Low-Rank Adaptation, LoRA）` 是目前 PEFT 家族中最成功、应用最广泛的技术之一。

### 3.1 核心假设：权重的低秩更新

在设计微调策略时，一个看似合理的思路是：先通过分析找到模型处理特定任务时“最活跃”的神经元或路径，然后针对性地修改它们。

然而，LoRA 的研究者们提出并验证了一个更具普适性和数学优雅性的假设：**预训练大模型在适应新任务时，其权重矩阵的变化是“低秩 (Low-Rank)”的**。通俗地讲，微调所需的“修改量” `ΔW`，其所包含的有效信息是高度冗余和可压缩的。

### 3.2 LoRA 的工作机制：优雅的数学魔术

对于任意一个权重矩阵 `W`，全量微调的目标是学习到一个更新后的矩阵 `W' = W + ΔW`。LoRA 的精髓在于，它不直接学习巨大的 `ΔW`，而是将其近似分解为两个非常瘦长的低秩矩阵 `A` 和 `B` 的乘积：
$$\Delta W \approx B \cdot A$$

这个分解带来了惊人的参数效率提升。以 Transformer 中的一个 `4096 x 4096` 维度的权重矩阵为例：
* 该矩阵 `W` 的总参数量约为 1677 万。
* 若选择一个很小的秩 `r = 8`，那么矩阵 `A` 的维度为 `8 x 4096`，矩阵 `B` 的维度为 `4096 x 8`。
* LoRA 需要训练的总参数量仅为 `(4096 * 8) + (8 * 4096) = 65,536`。

参数量从近 1700 万骤降至 6.5 万，仅为直接修改所需参数量的 **0.4%**。在模型前向传播时，数据流会“兵分两路”：
$$h = Wx + (B \cdot A)x$$
* **主路**：`Wx`，通过原始、被冻结的权重矩阵，完整保留了模型的通用能力。
* **旁路 (Bypass)**：`(BA)x`，通过新增的、可训练的低秩“适配器”，为模型注入新任务的特定知识。

最终，两条路径的输出相加，即完成了对模型行为的精准“微调”。

### 3.3 实践细节与深度洞察

**权重与 LoRA 适配器的独立性**：可以将多层的 Transformer 架构想象成一条高度专业化的“流水线工厂”。每一层（Block）都是一个拥有独特技能（独立的 `Wq, Wk, Wv` 等权重矩阵）的专家车间，负责处理不同层级的抽象特征。因此，为实现精准微调，每个目标权重矩阵都应配备一套独立的 LoRA 适配器 `(A, B)` 对，这保证了“培训”的针对性。

**训练循环的可视化**：
1.  **设置阶段**：训练开始前，遍历模型所有原始参数 `W`，将其 `requires_grad` 属性设置为 `False`。此举会告知深度学习框架在反向传播时完全“跳过”对这些参数的梯度计算，从而极大地节省算力和显存。同时，将所有新建的 `A` 和 `B` 矩阵标记为可训练。
2.  **前向传播**：数据输入，同时流经“主路”和“旁路”，并将结果汇总。
3.  **计算损失**：将模型输出与真实标签（Ground Truth）进行比较，计算出损失值。
4.  **反向传播**：损失值将“修正指令”（梯度）从后向前传播。由于 `W` 被标记为不可训练，梯度计算会被高效跳过。
5.  **参数更新**：优化器（如 AdamW）只接收并更新 `A` 和 `B` 这两个低秩矩阵的参数。

**关键超参数 `r` (rank) 和 `alpha`**：
* `r` (秩)：定义了 LoRA 适配器的“容量”，即“知识补丁”的大小。`r` 越大，可训练参数越多，模型的学习和拟合能力越强，但相应的计算成本和过拟合风险也越高。`r` 的选择是模型效果与资源成本之间的权衡 (Trade-off)。
* `alpha`：这是一个缩放因子。实践中，旁路的输出通常会经过一个缩放操作，完整的公式为 `h = Wx + (alpha / r) * (B \cdot A)x`。`alpha` 可以被看作是一种归一化手段，用于调整“旁路”对最终输出的贡献权重。一个常见的做法是设置 `alpha = r` 或 `alpha = 2 * r`。

### 3.4 LoRA 的核心优势总结

* **极低的训练成本**：显著降低了微调对硬件的依赖，使在消费级设备上微调大模型成为可能。
* **高效的存储与部署**：微调后，只需保存几十 MB 甚至几 MB 大小的适配器文件，而非一个数十 GB 的完整模型副本。这使得可以为不同任务训练多个适配器，并在推理时按需加载，实现“一模多用”。
* **保留通用能力**：由于主体模型被冻结，LoRA 在很大程度上避免了“灾难性遗忘”问题，能很好地保留模型的通用知识。

## 4. 待续的探索：更广阔的微调世界

掌握 LoRA 是进入现代大模型应用领域的关键一步，但微调的世界仍在飞速发展。以下方向值得进一步探索：

* **QLoRA: 极致的效率魔法**：一项突破性技术，它通过引入新的 4-bit 数据类型 (NF4) 和双重量化 (Double Quantization) 等创新，成功地解决了在 4-bit 量化模型上进行稳定训练的难题，进一步将硬件门槛降至极限。
* **微调的实践工作流：数据为王**：深入理解“Garbage In, Garbage Out”原则。学习如何清洗、构建和格式化高质量的指令微调数据集，是决定微调成败的最关键因素之一。
* **PEFT 家族的其他成员**：了解如 `Prompt Tuning`（只训练添加到输入端的“虚拟提示词”）、`Prefix Tuning` 或 `Adapter`（在 Transformer 层之间插入小型可训练模块）等其他 PEFT 方法，能为解决不同场景下的问题提供更丰富的工具箱。