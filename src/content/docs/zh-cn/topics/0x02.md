---
title: 0x02 - LLM 量化的不同方法 - GPTQ/AWQ 等
description: LLM 量化的不同方法 - GPTQ/AWQ 等
---


<!-- # 大语言模型量化技术深度探索笔记 -->

## 引言：为何需要量化

大语言模型（Large Language Model, LLM）的规模正以惊人的速度增长，其参数量动辄数十亿乃至上千亿。这带来了两大核心挑战：巨大的存储开销和高昂的计算资源需求。`模型量化（Model Quantization）`技术应运而生，它旨在通过降低模型参数的数值精度，在可接受的性能损失范围内，显著提升模型的存储和计算效率，从而让在消费级硬件上部署和运行高性能 LLM 成为可能。

本笔记旨在系统性梳理 LLM 量化的核心概念、主流技术、前沿进展及实践考量。

## 核心概念：区分方法、格式与工具

在深入算法之前，精确区分几个时常混淆的概念至关重要。

* **量化算法 (Quantization Algorithm)**
    这是量化技术的核心，定义了如何将高精度数值（如 32 位浮点数 `FP32` 或 16 位浮点数 `FP16`）映射到低精度数值（如 8 位整数 `INT8` 或 4 位整数 `INT4`）的一套数学规则与流程。我们后续讨论的 GPTQ、AWQ 等均属此类。其核心挑战在于设计一套映射方案，以最小化“精度压缩”过程中引入的`量化误差（Quantization Error）`。

* **文件格式 (File Format)**
    这是一种标准化的数据“容器”，用于存储被量化后的模型权重及相关的元数据。元数据至关重要，它包含了如何将低精度数据“解压缩”回可计算数值的指令，例如`缩放因子（Scaling Factor）`和`零点（Zero-point）`。一个著名的实例是 `GGUF（GPT-Generated Unified Format）`，它并非一种量化算法，而是专为 `llama.cpp` 项目设计的文件格式，能够封装用不同量化算法（如 k-quants）处理过的模型，因其对 CPU 和消费级硬件的友好性而广泛流行。

* **工具与框架 (Tools & Frameworks)**
    这是实现和应用量化算法的软件库。例如，`llama.cpp` 是一个实现了 GGUF 格式解析和高效 CPU 推理的 C++ 库。`Hugging Face Optimum` 则提供了在统一接口下调用多种量化算法（如 GPTQ）的便捷工具。

## 量化算法的核心流派与演进

当前主流的量化算法大多属于`后训练量化（Post-Training Quantization, PTQ）`的范畴。PTQ 指的是在模型已经完成全部预训练和微调之后，再对其进行量化处理。此方法成本低、效率高，因为它无需重新训练模型。

### PTQ 的基础：标量量化与分组

`标量量化（Scalar Quantization）`是最基础的量化思想，它独立地处理每一个权重值。其数学表示为：
$$r = \text{round}(S \cdot q + z)$$
其中，$r$ 是原始高精度值，$q$ 是量化后的低精度整数，$S$ 是缩放因子，$z$ 是零点。挑战在于为权重找到最优的 $S$ 和 $z$。

为了在效率和精度之间取得更好的平衡，`分组量化（Grouped Quantization）`被广泛采用。它将权重张量沿某个维度（通常是输入通道）分割成若干小组（例如 `group_size = 128`），并为每个小组计算一套独享的 $S$ 和 $z$。这样做的好处是，相比为每个权重都计算 $S$ 和 $z$，它极大地减少了元数据的存储开销；相比整个张量共享一套 $S$ 和 $z$，它又保留了更多的局部数值特征，精度更高。

### 主流 PTQ 算法详解

* GPTQ（Generative Pre-trained Transformer Quantizer）
    * **核心思想**：GPTQ 是一种基于近似二阶信息的量化方法，它将量化视为一种逐层进行的优化问题。其关键在于**误差补偿**机制：在量化一个权重块后，它会立即更新所有尚未被量化的权重，以此来抵消刚刚产生的量化误差。这种顺序依赖、迭代修正的思路，使其能达到较高的量化精度。
    * **实践考量**：由于其复杂的迭代更新过程，GPTQ 的量化过程本身较慢，且需要消耗较多的计算资源。它还需要一小部分`校准数据集（Calibration Dataset）`来执行量化，以便在模拟真实推理的过程中收集做出更优决策所需的统计信息。

* AWQ（Activation-aware Weight Quantization）
    * **核心思想**：AWQ 的洞见在于，模型中并非所有权重都同等重要。它认为，那些与数值较大的`激活值（Activation）`相乘的权重，对模型的最终输出影响更大。因此，AWQ 的策略并非保护数值大的权重，而是**保护那些对模型性能至关重要的“显著权重”（Salient Weights）**。它通过分析校准数据集上的激活值分布，识别出这些重要权重，并在量化过程中给予它们更高的保护（即更小的量化误差）。
    * **实践考量**：AWQ 的量化速度远快于 GPTQ，因为它不需要迭代优化。其量化产物与 `vLLM` 等推理引擎结合紧密，能实现非常高的推理吞吐量，因此在云端服务部署中备受青睐。

* HQQ（Half-Quadratic Quantization）
    * **核心思想**：HQQ 将量化问题重新定义为一个纯粹的数学优化问题，其目标是找到一个低精度矩阵，使其与原始高精度矩阵之间的`范数（Norm）`距离最小。它采用一种高效的“半二次方优化”算法来求解。
    * **实践考量**：HQQ 最引人注目的特点是**无需校准数据**，因为它完全不关心权重与激活值的交互，只专注于权重本身的保真度。这使得它的量化过程极快，且应用场景非常灵活，尤其适用于无法获取合适校准数据的模型。

### 前沿方向：挑战极低比特量化

当量化目标进入 2-bit 或 3-bit 的极限领域时，传统的标量量化方法精度损失巨大，更先进的`向量量化（Vector Quantization, VQ）`思想应运而生。

* AQLM（Additive Quantization of Language Models）
    * **核心思想**：AQLM 不再单独量化每个数值，而是将一组权重（一个向量）作为一个整体进行处理。它预先为模型学习一套共享的、由多个`密码本（Codebooks）`构成的“积木库”。任何一个原始向量，都可以通过从每个密码本中各选一个“积木向量”并相加来进行高度近似。模型最终只需存储指向这些积木的、极度压缩的“密码”（索引）即可。
    * **类比**：该思想与 `ZIP` 压缩有哲学上的共通之处，即用“引用/索引”代替“原文”，但 AQLM 是一种`有损压缩（Lossy Compression）`，而 ZIP 是`无损压缩（Lossless Compression）`。

* QuIP#（Quantization with Incoherence Processing, Sharp）
    * **核心思想**：QuIP# 认为，如果权重矩阵的结构是“杂乱无章”的（非相干），量化误差就不会集中在少数关键维度上，从而降低损害。因此，它在量化前，先用一个`随机正交矩阵（Random Orthogonal Matrix）`对权重进行变换，主动将其“打乱”，量化后再通过其逆变换恢复。这一过程确保了量化误差被均匀地分散开。

## 终极前沿：BitNet 与计算范式革命

`BitNet b1.58` 代表了比量化更激进的方向，它旨在重塑 LLM 的底层计算架构。

* **1.58-bit 权重**：其名称源于三元系统 `{-1, 0, +1}` 在均匀分布下的信息熵。在硬件层面，每个权重被量化为这三个整数值之一。
* **协同量化**：它不仅量化权重，也对激活值进行精巧的 8-bit 动态量化。通过将权重和激活值都转换为整数，它成功地将模型中原本最昂贵的**矩阵乘法运算，完全替换为了等效的整数加法运算**。
* **影响**：这一转变极大地降低了计算的能耗和延迟，并催生了设计以加法为核心的新型 AI 芯片的可能性，可能从根本上改变 LLM 的成本结构。
* **归一化问题**：对于量化中“归一化导致张量间大小关系丢失”的疑虑，解决方案在于**保留并传递缩放因子**。系统将数值的“大小”信息（存入缩放因子）与“形状”信息（存入量化整数）分离，在进行高效整数计算后，再通过缩放因子将“大小”信息重新融合，确保了模型层与层之间动态范围的正确性。

## 实践中的智慧与考量

* **如何评价量化质量**
    * **困惑度（Perplexity, PPL）**：衡量语言流畅度的基础指标，越低越好。
    * **标准能力基准**：通过 `MMLU` (综合知识), `GSM8K` (数学), `HumanEval` (代码) 等标准测试集，评估模型在具体任务上的表现，这是更具实践意义的度量。

* **挑战与权衡**
    * **模型敏感度**：不同模型架构对量化的耐受度不同。
    * **延迟与吞吐量**：需根据应用场景（实时聊天 vs. 离线处理）权衡`首字延迟（Time-to-First-Token, TTFT）`和`吞吐量（Tokens/sec）`。
    * **长文本累积误差**：量化误差在长序列处理中可能被放大，是当前的研究热点。

* **量化的边界**
    * **不应量化的场景**：在追求极致精度的科研、对可靠性要求极高的医疗等领域，以及模型训练和微调阶段（QAT 除外），应使用全精度模型。量化是优化效率的工具，而非提升性能上限的手段。
